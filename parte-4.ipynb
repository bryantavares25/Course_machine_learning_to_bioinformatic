{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Multilayer Perceptron: Fit and evaluate a model\n\nUsing the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n\nIn this section, we will fit and evaluate a simple Multilayer Perceptron model.","metadata":{}},{"cell_type":"markdown","source":"**Multilayer Perceptron**\n","metadata":{}},{"cell_type":"markdown","source":"**Ler Dados**\n\n**Separar Dados em Treinamento e Teste 70/30**","metadata":{}},{"cell_type":"code","source":"import joblib\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ntitanic = pd.read_csv('../input/titanic-cleaned/titanic_cleaned.csv')\ntitanic.head()\nfeatures = titanic.drop('Survived', axis=1)\nlabels = titanic['Survived']\nX_train, X_test, y_train, y_test = train_test_split(features, labels, \n                                                    test_size=0.3, random_state=42)\nfor dataset in [y_train, y_test]:\n    print(round(len(dataset) / len(labels), 2), len(dataset))\n\nprint(\"Train:\",(y_train.values.tolist()).count(0),\"(non-survive)\",(y_train.values.tolist()).count(1),\"(survive)\")\nprint(\"Test :\",(y_test.values.tolist()).count(0),\"(non-survive)\",(y_test.values.tolist()).count(1),\"(survive)\")\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:16:06.478664Z","iopub.execute_input":"2021-07-07T14:16:06.479319Z","iopub.status.idle":"2021-07-07T14:16:06.862839Z","shell.execute_reply.started":"2021-07-07T14:16:06.479194Z","shell.execute_reply":"2021-07-07T14:16:06.861632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PCA Treinamento","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom sklearn.decomposition import PCA\nX = X_train[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Family_cnt\", \"Cabin_ind\"]]\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nfig = px.scatter(components, x=0, y=1, color=y_train)\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PCA TESTE","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom sklearn.decomposition import PCA\nX = X_test[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Family_cnt\", \"Cabin_ind\"]]\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nfig = px.scatter(components, x=0, y=1, color=y_test)\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cm(tn,fp,fn,tp):   \n    import numpy as np\n    import matplotlib.pyplot as plt\n    import seaborn as sn\n    import pandas as pd\n    import seaborn as sns\n    import math\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n    import matplotlib as mpl\n    mpl.style.use('seaborn')\n    conf_arr = np.array([[tn,fp],[fn,tp]])\n    sum = conf_arr.sum()\n    df_cm = pd.DataFrame(conf_arr,\n      index = ['Non-Survive', 'Survive'],\n      columns = ['Non-Survive', 'Survive'])\n    fig = plt.figure()\n    plt.clf()\n    ax = fig.add_subplot(111)\n    ax.set_aspect(1)\n    cmap=sns.color_palette(\"flare\", as_cmap=True)\n    res = sn.heatmap(df_cm, annot=True, vmin=0.0, vmax=100.0, fmt='.0f', \n                     cmap=cmap, annot_kws={\"size\": 30})\n    plt.yticks([0.5,1.5], [ 'Non-Survive', 'Survive'],va='center',fontsize=18)\n    plt.xticks(fontsize=18)\n    plt.xlabel('Predicted',fontsize=20)\n    plt.ylabel('True',fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:16:10.319247Z","iopub.execute_input":"2021-07-07T14:16:10.319678Z","iopub.status.idle":"2021-07-07T14:16:10.329836Z","shell.execute_reply.started":"2021-07-07T14:16:10.319635Z","shell.execute_reply":"2021-07-07T14:16:10.32878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import confusion_matrix\n    \nmodel = MLPClassifier(hidden_layer_sizes=(10, 10), solver='sgd',random_state = 1)\npred=model.fit(X_train, y_train).predict(X_test)\ntn,fp,fn,tp = (confusion_matrix(y_test,pred,labels=[0,1]).ravel())\nprint (tn,fp,fn,tp)\ncm(tn,fp,fn,tp)\n\nprint(\"Especificidade:\",round(tn/(tn+fp),3))\nprint(\"Sensibilidade/Recall:\",round(tp/(tp+fn),3))\nprint(\"Precisão\",round(tp/(tp+fp),3))\nprint(\"Acurácia:\",round((tp+tn)/(tn+fp+fn+tp),3))\nprint(\"F1-score:\",round(2*((tp/(tp+fn)) * (tp/(tp+fp))) / ((tp/(tp+fn)) + (tp/(tp+fp))),3))\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:16:36.547362Z","iopub.execute_input":"2021-07-07T14:16:36.547924Z","iopub.status.idle":"2021-07-07T14:16:36.937074Z","shell.execute_reply.started":"2021-07-07T14:16:36.547888Z","shell.execute_reply":"2021-07-07T14:16:36.936252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import confusion_matrix\n\n\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nX = X_train[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Family_cnt\", \"Cabin_ind\"]]\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nfig = px.scatter(components, x=0, y=1, color=y_train)\nfig.show()\n\nprint(\"Train:\",(y_train.values.tolist()).count(0),\"(non-survive)\",(y_train.values.tolist()).count(1),\"(survive)\")\nprint (\"Shannon Entropy:\", balance(y_train))\n\n\nparam_grid = [{'activation': ['logistic', 'tanh', 'relu'],\n             'solver': ['sgd', 'adam'],\n             'learning_rate_init': [0.001, 0.01],\n             'hidden_layer_sizes': [(10, 10, 2), (4, 4), (5, 10, 5)],\n             'random_state': [1]}]\n#model = MLPClassifier()\nmodel = GridSearchCV(MLPClassifier(),param_grid,refit=True,verbose=0)\n\npred=model.fit(X_train, y_train).predict(X_test)\ntn,fp,fn,tp = (confusion_matrix(y_test,pred,labels=[0,1]).ravel())\nprint (tn,fp,fn,tp)\n\nprint(\"Especificidade:\",round(tn/(tn+fp),3))\nprint(\"Sensibilidade/Recall:\",round(tp/(tp+fn),3))\nprint(\"Precisão\",round(tp/(tp+fp),3))\nprint(\"Acurácia:\",round((tp+tn)/(tn+fp+fn+tp),3))\nprint(\"F1-score:\",round(2*((tp/(tp+fn)) * (tp/(tp+fp))) / ((tp/(tp+fn)) + (tp/(tp+fp))),3))\ncm(tn,fp,fn,tp)\n\nmodel.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:23:04.103967Z","iopub.execute_input":"2021-07-07T14:23:04.104714Z","iopub.status.idle":"2021-07-07T14:23:54.447447Z","shell.execute_reply.started":"2021-07-07T14:23:04.104672Z","shell.execute_reply":"2021-07-07T14:23:54.446421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Shannon entropy as a measure of balance\n\nOn a data set of n instances, if you have k classes of size ci you can compute entropy as follows:\n$H = -\\sum_{ i = 1}^k \\frac{c_i}{n} \\log{ \\frac{c_i}{n}}$.\nThis is equal to:\n\n0 when there is one single class. In other words, it tends to 0 when your data set is very unbalanced\nlogk when all your classes are balanced of the same size $\\frac{n}{k}$\nTherefore, you could use the following measure of Balance for a data set:\n$\\mbox{Balance} = \\frac{H}{\\log{k}} = \\frac{-\\sum_{ i = 1}^k \\frac{c_i}{n} \\log{ \\frac{c_i}{n}}.  } {\\log{k}}$\nwhich is equal to:\n\n0 for a unbalanced data set\n\n1 for a balanced data set\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport scipy.stats\n\n\ndef balance(seq):\n    from collections import Counter\n    from numpy import log\n    \n    n = len(seq)\n    classes = [(clas,float(count)) for clas,count in Counter(seq).items()]\n    k = len(classes)\n    \n    H = -sum([ (count/n) * log((count/n)) for clas,count in classes]) #shannon entropy\n    return H/log(k)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:22:59.099084Z","iopub.execute_input":"2021-07-07T14:22:59.099558Z","iopub.status.idle":"2021-07-07T14:22:59.107108Z","shell.execute_reply.started":"2021-07-07T14:22:59.099514Z","shell.execute_reply":"2021-07-07T14:22:59.10572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train:\",(y_train.values.tolist()).count(0),\"(non-survive)\",(y_train.values.tolist()).count(1),\"(survive)\")\nprint (\"Shannon Entropy:\", balance(y_train))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:26:44.526521Z","iopub.execute_input":"2021-07-07T14:26:44.527023Z","iopub.status.idle":"2021-07-07T14:26:44.535224Z","shell.execute_reply.started":"2021-07-07T14:26:44.526974Z","shell.execute_reply":"2021-07-07T14:26:44.534055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Oversampling \n\nRandom oversampling involves randomly selecting examples from the minority class, with replacement, and adding them to the training dataset. Random undersampling involves randomly selecting examples from the majority class and deleting them from the training dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nimport warnings\nfrom imblearn.over_sampling import RandomOverSampler\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import confusion_matrix\n\nsm = RandomOverSampler(random_state=0)\nX_trainB,  y_trainB = sm.fit_resample(X_train,y_train)\n\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nX = X_trainB[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Family_cnt\", \"Cabin_ind\"]]\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nfig = px.scatter(components, x=0, y=1, color=y_trainB)\nfig.show()\n\nprint(\"Train:\",(y_trainB.values.tolist()).count(0),\"(non-survive)\",(y_trainB.values.tolist()).count(1),\"(survive)\")\nprint (\"Shannon Entropy:\", balance(y_trainB))\n\n\nparam_grid = [{'activation': ['logistic', 'tanh', 'relu'],\n             'solver': ['sgd', 'adam'],\n             'learning_rate_init': [0.001, 0.01],\n             'hidden_layer_sizes': [(10, 10, 2), (4, 4), (5, 10, 5)],\n             'random_state': [1]}]\n#model = MLPClassifier()\nmodel = GridSearchCV(MLPClassifier(),param_grid,refit=True,verbose=0)\n\npred=model.fit(X_trainB, y_trainB).predict(X_test)\ntn,fp,fn,tp = (confusion_matrix(y_test,pred,labels=[0,1]).ravel())\nprint (tn,fp,fn,tp)\n\nprint(\"Especificidade:\",round(tn/(tn+fp),3))\nprint(\"Sensibilidade/Recall:\",round(tp/(tp+fn),3))\nprint(\"Precisão\",round(tp/(tp+fp),3))\nprint(\"Acurácia:\",round((tp+tn)/(tn+fp+fn+tp),3))\nprint(\"F1-score:\",round(2*((tp/(tp+fn)) * (tp/(tp+fp))) / ((tp/(tp+fn)) + (tp/(tp+fp))),3))\ncm(tn,fp,fn,tp)\n\nmodel.best_estimator_\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:33:41.700369Z","iopub.execute_input":"2021-07-07T14:33:41.700801Z","iopub.status.idle":"2021-07-07T14:34:38.569Z","shell.execute_reply.started":"2021-07-07T14:33:41.700767Z","shell.execute_reply":"2021-07-07T14:34:38.567791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Undersampling \n\nRandom undersampling involves randomly selecting examples from the majority class to delete from the training dataset.\n\nThis has the effect of reducing the number of examples in the majority class in the transformed version of the training dataset. This process can be repeated until the desired class distribution is achieved, such as an equal number of examples for each class.","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nimport warnings\nfrom imblearn.under_sampling import RandomUnderSampler\n\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import confusion_matrix\n\nsm = RandomUnderSampler(random_state=0)\nX_trainB,  y_trainB = sm.fit_resample(X_train,y_train)\n\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nX = X_trainB[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Family_cnt\", \"Cabin_ind\"]]\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nfig = px.scatter(components, x=0, y=1, color=y_trainB)\nfig.show()\n\nprint(\"Train:\",(y_trainB.values.tolist()).count(0),\"(non-survive)\",(y_trainB.values.tolist()).count(1),\"(survive)\")\nprint (\"Shannon Entropy:\", balance(y_trainB))\n\n\nparam_grid = [{'activation': ['logistic', 'tanh', 'relu'],\n             'solver': ['sgd', 'adam'],\n             'learning_rate_init': [0.001, 0.01],\n             'hidden_layer_sizes': [(10, 10, 2), (4, 4), (5, 10, 5)],\n             'random_state': [1]}]\n#model = MLPClassifier()\nmodel = GridSearchCV(MLPClassifier(),param_grid,refit=True,verbose=0)\n\npred=model.fit(X_trainB, y_trainB).predict(X_test)\ntn,fp,fn,tp = (confusion_matrix(y_test,pred,labels=[0,1]).ravel())\nprint (tn,fp,fn,tp)\n\nprint(\"Especificidade:\",round(tn/(tn+fp),3))\nprint(\"Sensibilidade/Recall:\",round(tp/(tp+fn),3))\nprint(\"Precisão\",round(tp/(tp+fp),3))\nprint(\"Acurácia:\",round((tp+tn)/(tn+fp+fn+tp),3))\nprint(\"F1-score:\",round(2*((tp/(tp+fn)) * (tp/(tp+fp))) / ((tp/(tp+fn)) + (tp/(tp+fp))),3))\ncm(tn,fp,fn,tp)\n\nmodel.best_estimator_\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:43:06.549791Z","iopub.execute_input":"2021-07-07T14:43:06.550229Z","iopub.status.idle":"2021-07-07T14:43:43.397185Z","shell.execute_reply.started":"2021-07-07T14:43:06.550192Z","shell.execute_reply":"2021-07-07T14:43:43.396154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Synthetic Minority Oversampling Technique\n\nSMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n\nSpecifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nimport warnings\nfrom imblearn.over_sampling import SMOTE\n\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import confusion_matrix\n\nsm = SMOTE(random_state=0)\nX_trainB,  y_trainB = sm.fit_resample(X_train,y_train)\n\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nX = X_trainB[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Family_cnt\", \"Cabin_ind\"]]\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nfig = px.scatter(components, x=0, y=1, color=y_trainB)\nfig.show()\n\nprint(\"Train:\",(y_trainB.values.tolist()).count(0),\"(non-survive)\",(y_trainB.values.tolist()).count(1),\"(survive)\")\nprint (\"Shannon Entropy:\", balance(y_trainB))\n\nparam_grid = [{'activation': ['logistic', 'tanh', 'relu'],\n             'solver': ['sgd', 'adam'],\n             'learning_rate_init': [0.001, 0.01],\n             'hidden_layer_sizes': [(10, 10, 2), (4, 4), (5, 10, 5)],\n             'random_state': [1]}]\n#model = MLPClassifier()\nmodel = GridSearchCV(MLPClassifier(),param_grid,refit=True,verbose=0)\n\npred=model.fit(X_trainB, y_trainB).predict(X_test)\ntn,fp,fn,tp = (confusion_matrix(y_test,pred,labels=[0,1]).ravel())\nprint (tn,fp,fn,tp)\n\nprint(\"Especificidade:\",round(tn/(tn+fp),3))\nprint(\"Sensibilidade/Recall:\",round(tp/(tp+fn),3))\nprint(\"Precisão\",round(tp/(tp+fp),3))\nprint(\"Acurácia:\",round((tp+tn)/(tn+fp+fn+tp),3))\nprint(\"F1-score:\",round(2*((tp/(tp+fn)) * (tp/(tp+fp))) / ((tp/(tp+fn)) + (tp/(tp+fp))),3))\ncm(tn,fp,fn,tp)\n\nmodel.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:48:29.325891Z","iopub.execute_input":"2021-07-07T14:48:29.3263Z","iopub.status.idle":"2021-07-07T14:49:28.07593Z","shell.execute_reply.started":"2021-07-07T14:48:29.326264Z","shell.execute_reply":"2021-07-07T14:49:28.074685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combination of SMOTE and Tomek Links Undersampling\n\nSMOTE is an oversampling method that synthesizes new plausible examples in the minority class.\n\nTomek Links refers to a method for identifying pairs of nearest neighbors in a dataset that have different classes. Removing one or both of the examples in these pairs (such as the examples in the majority class) has the effect of making the decision boundary in the training dataset less noisy or ambiguous.\n\nSpecifically, first the SMOTE method is applied to oversample the minority class to a balanced distribution, then examples in Tomek Links from the majority classes are identified and removed.","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nimport warnings\nfrom imblearn.combine import SMOTETomek\n\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import confusion_matrix\n\nsm = SMOTETomek(random_state=0)\nX_trainB,  y_trainB = sm.fit_resample(X_train,y_train)\n\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nX = X_trainB[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Family_cnt\", \"Cabin_ind\"]]\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nfig = px.scatter(components, x=0, y=1, color=y_trainB)\nfig.show()\n\n\n\nprint(\"Train:\",(y_trainB.values.tolist()).count(0),\"(non-survive)\",(y_trainB.values.tolist()).count(1),\"(survive)\")\nprint (\"Shannon Entropy:\", balance(y_trainB))\n\nparam_grid = [{'activation': ['logistic', 'tanh', 'relu'],\n             'solver': ['sgd', 'adam'],\n             'learning_rate_init': [0.001, 0.01],\n             'hidden_layer_sizes': [(10, 10, 2), (4, 4), (5, 10, 5)],\n             'random_state': [1]}]\n#model = MLPClassifier()\nmodel = GridSearchCV(MLPClassifier(),param_grid,refit=True,verbose=0)\n\npred=model.fit(X_trainB, y_trainB).predict(X_test)\ntn,fp,fn,tp = (confusion_matrix(y_test,pred,labels=[0,1]).ravel())\nprint (tn,fp,fn,tp)\n\nprint(\"Especificidade:\",round(tn/(tn+fp),3))\nprint(\"Sensibilidade/Recall:\",round(tp/(tp+fn),3))\nprint(\"Precisão\",round(tp/(tp+fp),3))\nprint(\"Acurácia:\",round((tp+tn)/(tn+fp+fn+tp),3))\nprint(\"F1-score:\",round(2*((tp/(tp+fn)) * (tp/(tp+fp))) / ((tp/(tp+fn)) + (tp/(tp+fp))),3))\ncm(tn,fp,fn,tp)\n\nmodel.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:52:59.168884Z","iopub.execute_input":"2021-07-07T14:52:59.169317Z","iopub.status.idle":"2021-07-07T14:53:51.089605Z","shell.execute_reply.started":"2021-07-07T14:52:59.16928Z","shell.execute_reply":"2021-07-07T14:53:51.088441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nimport warnings\nfrom imblearn.over_sampling import ADASYN\n\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import confusion_matrix\n\nsm = ADASYN(random_state=0)\nX_trainB,  y_trainB = sm.fit_resample(X_train,y_train)\n\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nX = X_trainB[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Family_cnt\", \"Cabin_ind\"]]\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nfig = px.scatter(components, x=0, y=1, color=y_trainB)\nfig.show()\n\nprint(\"Train:\",(y_trainB.values.tolist()).count(0),\"(non-survive)\",(y_trainB.values.tolist()).count(1),\"(survive)\")\nprint (\"Shannon Entropy:\", balance(y_trainB))\n\nparam_grid = [{'activation': ['logistic', 'tanh', 'relu'],\n             'solver': ['sgd', 'adam'],\n             'learning_rate_init': [0.001, 0.01],\n             'hidden_layer_sizes': [(10, 10, 2), (4, 4), (5, 10, 5)],\n             'random_state': [1]}]\n#model = MLPClassifier()\nmodel = GridSearchCV(MLPClassifier(),param_grid,refit=True,verbose=0)\n\npred=model.fit(X_trainB, y_trainB).predict(X_test)\ntn,fp,fn,tp = (confusion_matrix(y_test,pred,labels=[0,1]).ravel())\nprint (tn,fp,fn,tp)\n\nprint(\"Especificidade:\",round(tn/(tn+fp),3))\nprint(\"Sensibilidade/Recall:\",round(tp/(tp+fn),3))\nprint(\"Precisão\",round(tp/(tp+fp),3))\nprint(\"Acurácia:\",round((tp+tn)/(tn+fp+fn+tp),3))\nprint(\"F1-score:\",round(2*((tp/(tp+fn)) * (tp/(tp+fp))) / ((tp/(tp+fn)) + (tp/(tp+fp))),3))\ncm(tn,fp,fn,tp)\n\nmodel.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:56:19.851724Z","iopub.execute_input":"2021-07-07T14:56:19.852153Z","iopub.status.idle":"2021-07-07T14:57:13.88071Z","shell.execute_reply.started":"2021-07-07T14:56:19.852119Z","shell.execute_reply":"2021-07-07T14:57:13.879451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nimport warnings\nfrom imblearn.over_sampling import ADASYN\n\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nweights = np.linspace(0.0,0.99,200)\nsm = ADASYN(random_state=0)\nX_trainB,  y_trainB = sm.fit_resample(X_train,y_train)\n\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nX = X_trainB[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Family_cnt\", \"Cabin_ind\"]]\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nfig = px.scatter(components, x=0, y=1, color=y_trainB)\nfig.show()\n\nprint(\"Train:\",(y_trainB.values.tolist()).count(0),\"(non-survive)\",(y_trainB.values.tolist()).count(1),\"(survive)\")\nprint (\"Shannon Entropy:\", balance(y_trainB))\n\nparam_grid = [{'activation': ['logistic', 'tanh', 'relu'],\n             'solver': ['sgd', 'adam'],\n             'learning_rate_init': [0.001, 0.01],\n             'hidden_layer_sizes': [(10, 10, 2), (4, 4), (5, 10, 5)],\n            # 'class_weight': [{0:x, 1:1.0-x} for x in weights],\n             'random_state': [1]}]\n#model = MLPClassifier()\nmodel = GridSearchCV(MLPClassifier(),param_grid,refit=True,verbose=0)\n\npred=model.fit(X_trainB, y_trainB).predict(X_test)\ntn,fp,fn,tp = (confusion_matrix(y_test,pred,labels=[0,1]).ravel())\nprint (tn,fp,fn,tp)\n\nprint(\"Especificidade:\",round(tn/(tn+fp),3))\nprint(\"Sensibilidade/Recall:\",round(tp/(tp+fn),3))\nprint(\"Precisão\",round(tp/(tp+fp),3))\nprint(\"Acurácia:\",round((tp+tn)/(tn+fp+fn+tp),3))\nprint(\"F1-score:\",round(2*((tp/(tp+fn)) * (tp/(tp+fp))) / ((tp/(tp+fn)) + (tp/(tp+fp))),3))\ncm(tn,fp,fn,tp)\n\nmodel.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}